{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b39159",
   "metadata": {},
   "source": [
    "# **05 - Submit on Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from dataset import Dataset\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64271234",
   "metadata": {},
   "source": [
    "## **Normalization**\n",
    "\n",
    "Each year must produce the exact same column set regardless of its format group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e87ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import (\n",
    "    CANONICAL_CHARACTERISTICS,\n",
    "    CANONICAL_LOCATIONS,\n",
    "    CANONICAL_VEHICLES,\n",
    "    CANONICAL_USERS,\n",
    ")\n",
    "\n",
    "\n",
    "ds = Dataset(\"test\")\n",
    "assert ds.characteristics.columns.tolist() == CANONICAL_CHARACTERISTICS, \"test: characteristics columns mismatch\"\n",
    "assert ds.locations.columns.tolist() == CANONICAL_LOCATIONS, \"test: locations columns mismatch\"\n",
    "assert ds.vehicles.columns.tolist() == CANONICAL_VEHICLES, \"test: vehicles columns mismatch\"\n",
    "assert ds.users.columns.tolist() == CANONICAL_USERS, \"test : users columns mismatch\"\n",
    "n_acc = ds.characteristics[\"Num_Acc\"].nunique()\n",
    "\n",
    "print(f\"\\nTest normalized successfully. Found {n_acc:,} unique accidents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f11bfc",
   "metadata": {},
   "source": [
    "## **Build test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefe986",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Dataset(\"test\").merged()\n",
    "print(f\"Test set: {test.shape[0]:,} rows x {test.shape[1]} cols\")\n",
    "print(f\"Unique accidents: {test['Num_Acc'].nunique():,}\")\n",
    "print(f\"grav column (should be all NaN): {test['grav'].isna().all()}\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d1a6e",
   "metadata": {},
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573745c",
   "metadata": {},
   "source": [
    "### **Accident-level aggregation features**\n",
    "\n",
    "Count features derived from `groupby(\"Num_Acc\")`:\n",
    "- `nb_vehicules` — number of distinct vehicles involved\n",
    "- `nb_usagers` — total number of users (drivers, passengers, pedestrians)\n",
    "- `nb_pietons` — number of pedestrians (`catu == 3`)\n",
    "- `nb_occupants_vehicule` — number of non-pedestrian users per vehicle (`catu != 3`), to avoid counting pedestrians who are linked to a vehicle only because the accident involves them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42228a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per accident\n",
    "test[\"nb_vehicules\"] = test.groupby(\"Num_Acc\")[\"num_veh\"].transform(\"nunique\")\n",
    "test[\"nb_usagers\"]   = test.groupby(\"Num_Acc\")[\"catu\"].transform(\"count\")\n",
    "test[\"nb_pietons\"]   = test.groupby(\"Num_Acc\")[\"catu\"].transform(lambda x: (x == 3).sum())\n",
    "\n",
    "# Per vehicle: count only non-pedestrians (catu != 3)\n",
    "occupants = (\n",
    "    test[test[\"catu\"] != 3]\n",
    "    .groupby([\"Num_Acc\", \"num_veh\"])\n",
    "    .size()\n",
    "    .rename(\"nb_occupants_vehicule\")\n",
    ")\n",
    "test = test.merge(occupants, on=[\"Num_Acc\", \"num_veh\"], how=\"left\")\n",
    "test[\"nb_occupants_vehicule\"] = test[\"nb_occupants_vehicule\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e108d",
   "metadata": {},
   "source": [
    "### **Individual-level features**\n",
    "\n",
    "Derive per-user attributes from existing columns:\n",
    "- `age` — age at the time of the accident (`an - an_nais`). Values outside [0, 120] are set to NaN as data entry errors."
   ]
  },
  {
   "cell_type": "code",
   "id": "7o7mqz91wqa",
   "source": "test[\"age\"] = test[\"an\"] - test[\"an_nais\"]\ntest.loc[(test[\"age\"] < 0) | (test[\"age\"] > 120), \"age\"] = np.nan\n\nprint(f\"age NaN: {test['age'].isna().sum():,} ({test['age'].isna().mean():.2%})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "813b3051",
   "metadata": {},
   "source": [
    "### **Drop low-value columns**\n",
    "\n",
    "Remove free-text, identifier, and high-cardinality string columns that carry no direct predictive signal for a tree-based model:\n",
    "\n",
    "| Column | Reason |\n",
    "|--------|--------|\n",
    "| `voie` | Road name/number — free-text, very high cardinality |\n",
    "| `v1`, `v2` | Alphanumeric road index points — free-text identifiers |\n",
    "| `pr`, `pr1` | Milestone references (bornes kilométriques) — numeric road markers, not meaningful as features |\n",
    "| `com` | Commune code — inconsistent format across groups (3-digit vs full INSEE), high cardinality (~36k unique). Geographic info already captured by `dep` |\n",
    "\n",
    "Note: `adr`, `lat`, `long` were already dropped during normalization (free-text address and incompatible coordinate systems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"voie\", \"v1\", \"v2\", \"pr\", \"pr1\", \"com\"]\n",
    "test.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64416b9",
   "metadata": {},
   "source": [
    "### **Variable transformations**\n",
    "\n",
    "- `hrmn` (integer HHMM) → extract `hour` (0-23), then drop `hrmn`\n",
    "- `lartpc`, `larrout` — road widths stored as strings → convert to numeric\n",
    "- `an_nais` — redundant with `age` → drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from HHMM\n",
    "test[\"hour\"] = test[\"hrmn\"] // 100\n",
    "\n",
    "# Convert road widths to numeric\n",
    "for col in (\"lartpc\", \"larrout\"):\n",
    "    test[col] = pd.to_numeric(test[col], errors=\"coerce\")\n",
    "\n",
    "# Drop redundant columns\n",
    "test.drop(columns=[\"hrmn\", \"an_nais\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340220e7",
   "metadata": {},
   "source": [
    "### **Drop identifiers**\n",
    "\n",
    "`Num_Acc` and `num_veh` are row identifiers with no predictive value. Dropped after all groupby-based features have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f4224",
   "metadata": {},
   "outputs": [],
   "source": "# Save Num_Acc for submission, then drop identifiers and grav\nsubmission_ids = test[\"Num_Acc\"].copy()\ntest.drop(columns=[\"Num_Acc\", \"num_veh\", \"grav\"], inplace=True)\n\nprint(f\"Test features: {test.shape[1]} cols\")\nprint(f\"Submission IDs: {submission_ids.nunique():,} unique accidents\")"
  },
  {
   "cell_type": "markdown",
   "id": "kliiruj6fv",
   "source": "## **Load model & predict**\n\nLoad the saved LightGBM model, cast categorical features to the same dtype used during training, and predict `P(GRAVE=1)` for each row.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jostdo983o8",
   "source": "import lightgbm as lgb\n\n# Categorical features (same list as training)\nCAT_FEATURES = [\n    \"mois\", \"jour\", \"lum\", \"agg\", \"int\", \"atm\", \"col\", \"dep\",\n    \"catr\", \"circ\", \"vosp\", \"prof\", \"plan\", \"surf\", \"infra\", \"situ\",\n    \"senc\", \"catv\", \"obs\", \"obsm\", \"choc\", \"manv\",\n    \"place\", \"catu\", \"sexe\", \"trajet\", \"secu1\", \"locp\", \"actp\", \"etatp\",\n    \"hour\",\n]\n\nfor col in CAT_FEATURES:\n    test[col] = test[col].astype(\"category\")\n\n# Load model\nmodel = lgb.Booster(model_file=\"../models/lgbm_grave.txt\")\n\n# Predict P(GRAVE=1) per row\ntest[\"y_prob\"] = model.predict(test[model.feature_name()])\n\nprint(f\"Predictions: min={test['y_prob'].min():.4f}  max={test['y_prob'].max():.4f}  mean={test['y_prob'].mean():.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xlilqfe5ag",
   "source": "## **Aggregate to accident level & export**\n\nThe model predicts at the **user level** (one row per person). For submission we need one prediction per accident (`Num_Acc`). We take the **max probability** across all users of the same accident — consistent with the target definition (GRAVE=1 if *at least one* user is killed or hospitalized).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "atssb6pm0ed",
   "source": "# Aggregate: max proba per accident\nsubmission = (\n    pd.DataFrame({\"Num_Acc\": submission_ids, \"y_prob\": test[\"y_prob\"]})\n    .groupby(\"Num_Acc\")[\"y_prob\"]\n    .max()\n    .reset_index()\n    .rename(columns={\"y_prob\": \"GRAVE\"})\n)\n\nprint(f\"Submission: {submission.shape[0]:,} accidents\")\nprint(f\"GRAVE mean: {submission['GRAVE'].mean():.4f}\")\nsubmission.head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8yr8ljzhz76",
   "source": "output_path = \"../submissions/lgbm_grave.csv\"\nsubmission.to_csv(output_path, index=False)\nprint(f\"Saved to {output_path} ({submission.shape[0]:,} rows)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}